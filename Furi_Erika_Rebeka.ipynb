{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langgraph transformers langchain-community langchain-huggingface langchain_text_splitters chromadb pymupdf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXKl53VxbrND",
        "outputId": "e46c21ce-f458-4d5d-ba59-a6b50195cb7a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.2/151.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tempfile\n",
        "from typing import Annotated, Literal, Sequence, TypedDict\n",
        "import pprint\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.messages import BaseMessage, HumanMessage\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain.schema import AIMessage\n",
        "\n",
        "print(\"Setting up document processing...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99He6eZRb1-q",
        "outputId": "e607bcb5-bfb9-49a1-98a6-d12c1f5a3d78"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up document processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PDF processing and chunking\n",
        "pdf_path = \"/BERT_model.pdf\"\n",
        "loader = PyMuPDFLoader(pdf_path)\n",
        "documents = loader.load()\n",
        "print(f\"Loaded {len(documents)} pages from PDF\")\n",
        "\n",
        "# Split documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(documents)\n",
        "print(f\"Split into {len(doc_splits)} chunks\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LraA8a2Xb95N",
        "outputId": "85fe30ec-4175-4b6b-840f-4c7857593a74"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 16 pages from PDF\n",
            "Split into 152 chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding and indexing\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': 'cpu'}\n",
        ")\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name=\"pdf-rag-chroma\",\n",
        "    embedding=embeddings\n",
        ")\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 3}\n",
        ")\n",
        "\n",
        "retriever_tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    \"retrieve_pdf_content\",\n",
        "    \"Search and return information from the PDF document based on the query.\"\n",
        ")\n",
        "tools = [retriever_tool]\n"
      ],
      "metadata": {
        "id": "aOCVhWyxb33F"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "print(\"Loading microsoft/phi-1_5 model...\")\n",
        "model_id = \"microsoft/phi-1_5\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7k1HQxEfcMi5",
        "outputId": "fa6ba214-7298-47d1-848d-6d9f999f26f8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading microsoft/phi-1_5 model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phi_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.2,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "phi_llm = HuggingFacePipeline(pipeline=phi_pipeline)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETSWdbJ3cSCM",
        "outputId": "08af4a0e-f9ba-45c4-d4f8-3d2fbb7e1948"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]"
      ],
      "metadata": {
        "id": "Q2C3simIcUvb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define edge functions\n",
        "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
        "    \"\"\"\n",
        "    Determines whether the retrieved documents are relevant to the question.\n",
        "    \"\"\"\n",
        "    print(\"---CHECK RELEVANCE---\")\n",
        "\n",
        "    # Data model for grading\n",
        "    class grade(BaseModel):\n",
        "        \"\"\"Binary score for relevance check.\"\"\"\n",
        "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
        "\n",
        "    # Prompt for relevance checking\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"You are assessing if a document is relevant to a question.\n",
        "        Document: {context}\n",
        "        Question: {question}\n",
        "        If the document contains information related to the question, say 'yes', otherwise say 'no'.\"\"\",\n",
        "        input_variables=[\"context\", \"question\"],\n",
        "    )\n",
        "\n",
        "    # Simple relevance check with phi model\n",
        "    messages = state[\"messages\"]\n",
        "    question = messages[0].content\n",
        "    last_message = messages[-1]\n",
        "    docs = last_message.content\n",
        "\n",
        "    # Use phi model to check relevance\n",
        "    inputs = prompt.format(question=question, context=docs)\n",
        "    result = phi_llm.invoke(inputs)\n",
        "\n",
        "    # Extract yes/no from result\n",
        "    if \"yes\" in result.lower():\n",
        "        print(\"---DECISION: DOCS RELEVANT---\")\n",
        "        return \"generate\"\n",
        "    else:\n",
        "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
        "        return \"rewrite\"\n"
      ],
      "metadata": {
        "id": "aZgJdAjXceVw"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define node functions\n",
        "def agent(state):\n",
        "    \"\"\"\n",
        "    Invokes the agent model to generate a response based on the current state.\n",
        "    \"\"\"\n",
        "    print(\"---CALL AGENT---\")\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    # Create a prompt to decide if retrieval is needed\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"Based on the question, do you need to search for information in the document?\n",
        "        Question: {question}\n",
        "        Answer 'Yes' if you need to search, or provide a direct answer if you don't need to search.\"\"\",\n",
        "        input_variables=[\"question\"],\n",
        "    )\n",
        "\n",
        "    question = messages[0].content\n",
        "    inputs = prompt.format(question=question)\n",
        "    result = phi_llm.invoke(inputs)\n",
        "\n",
        "    # Check if retrieval is needed based on the response\n",
        "    if \"yes\" in result.lower():\n",
        "        # Return a tool call like message\n",
        "        return {\"messages\": [AIMessage(content=f\"Please use the retrieve_pdf_content tool with query: {question}\")]}\n",
        "    else:\n",
        "        # Return a direct answer\n",
        "        return {\"messages\": [HumanMessage(content=result)]}\n",
        "\n",
        "def rewrite(state):\n",
        "    \"\"\"\n",
        "    Transform the query to produce a better question.\n",
        "    \"\"\"\n",
        "    print(\"---TRANSFORM QUERY---\")\n",
        "    messages = state[\"messages\"]\n",
        "    question = messages[0].content\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"Please rewrite this question to make it more specific and searchable:\n",
        "        Question: {question}\n",
        "        Improved question:\"\"\",\n",
        "        input_variables=[\"question\"],\n",
        "    )\n",
        "\n",
        "    inputs = prompt.format(question=question)\n",
        "    result = phi_llm.invoke(inputs)\n",
        "\n",
        "    return {\"messages\": [HumanMessage(content=result)]}\n",
        "\n",
        "def generate(state):\n",
        "    \"\"\"\n",
        "    Generate answer based on retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---GENERATE ANSWER---\")\n",
        "    messages = state[\"messages\"]\n",
        "    question = messages[0].content\n",
        "    last_message = messages[-1]\n",
        "    docs = last_message.content\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"You are an assistant answering questions based on the provided document.\n",
        "\n",
        "        Context from document:\n",
        "        {context}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Provide a concise answer based only on the context provided. If the context doesn't contain relevant information, say \"I don't have enough information to answer this question.\"\n",
        "\n",
        "        Answer:\"\"\",\n",
        "        input_variables=[\"context\", \"question\"],\n",
        "    )\n",
        "\n",
        "    # Chain\n",
        "    inputs = prompt.format(context=docs, question=question)\n",
        "    result = phi_llm.invoke(inputs)\n",
        "\n",
        "    return {\"messages\": [HumanMessage(content=result)]}\n"
      ],
      "metadata": {
        "id": "vGYm-uXacipB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the graph\n",
        "print(\"Building the retrieval agent graph...\")\n",
        "workflow = StateGraph(AgentState)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rc64Pp57cmWG",
        "outputId": "7f6c3a51-c219-44e7-9d13-481d4136a367"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building the retrieval agent graph...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the nodes\n",
        "workflow.add_node(\"agent\", agent)\n",
        "retrieve = ToolNode([retriever_tool])\n",
        "workflow.add_node(\"retrieve\", retrieve)\n",
        "workflow.add_node(\"rewrite\", rewrite)\n",
        "workflow.add_node(\"generate\", generate)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2YQ2zxicomN",
        "outputId": "d2893454-6f13-4b87-b94f-b1486c8687c8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7e8f0a725290>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the edges\n",
        "workflow.add_edge(START, \"agent\")\n",
        "\n",
        "# Decide whether to retrieve\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    # More simplified condition for phi model\n",
        "    lambda x: \"retrieve_pdf_content\" in x[\"messages\"][-1].content if x[\"messages\"] else False,\n",
        "    {\n",
        "        True: \"retrieve\",\n",
        "        False: END\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clOWS4jQcpVl",
        "outputId": "dc761450-1796-42d9-83c6-359e424dec4b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7e8f0a725290>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Edges after retrieval\n",
        "workflow.add_conditional_edges(\n",
        "    \"retrieve\",\n",
        "    grade_documents,\n",
        "    {\n",
        "        \"generate\": \"generate\",\n",
        "        \"rewrite\": \"rewrite\"\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"generate\", END)\n",
        "workflow.add_edge(\"rewrite\", \"agent\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ma4elRBGcrS8",
        "outputId": "2dccdff7-64be-4eef-80bc-e8f9705cf73c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7e8f0a725290>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the graph\n",
        "graph = workflow.compile()\n"
      ],
      "metadata": {
        "id": "L411XKeoctPP"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to ask questions to the PDF\n",
        "def ask_pdf(question):\n",
        "    print(f\"\\nQuestion: {question}\")\n",
        "    print(\"Processing...\")\n",
        "\n",
        "    inputs = {\n",
        "        \"messages\": [\n",
        "            HumanMessage(content=question),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    response = \"\"\n",
        "    # Process the graph and get results\n",
        "    for output in graph.stream(inputs):\n",
        "        for key, value in output.items():\n",
        "            if key == \"generate\":\n",
        "                response = value[\"messages\"][0].content\n",
        "\n",
        "    print(\"\\nAnswer:\")\n",
        "    print(response)\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "T17vbgcMcvNS"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Your PDF is ready for questions!\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "\n",
        "ask_pdf(\"What is the Pre-training BERT?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "FUNBrEXhcxEB",
        "outputId": "3da84420-2501-41b5-aafa-55b52a2e1bb7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Your PDF is ready for questions!\n",
            "==================================================\n",
            "\n",
            "Question: What is the Pre-training BERT?\n",
            "Processing...\n",
            "---CALL AGENT---\n",
            "---CHECK RELEVANCE---\n",
            "---DECISION: DOCS RELEVANT---\n",
            "---GENERATE ANSWER---\n",
            "\n",
            "Answer:\n",
            "You are an assistant answering questions based on the provided document. \n",
            "        \n",
            "        Context from document:\n",
            "        Please use the retrieve_pdf_content tool with query: What is the Pre-training BERT?\n",
            "        \n",
            "        Question: What is the Pre-training BERT?\n",
            "        \n",
            "        Provide a concise answer based only on the context provided. If the context doesn't contain relevant information, say \"I don't have enough information to answer this question.\"\n",
            "        \n",
            "        Answer: The paper I'm currently reading talks about how researchers trained their model using a pre-trained tokenizer and pretrained weights before fine tuning it for specific tasks like language modeling or named entity recognition (NER). They used a popular pre-trained transformer architecture called 'Bert' which was developed by Google Brain Team in 2018 as part of the T5 family that consists of four models including Bert, GPT2, RoBERTa, XLNet etc., each having its own unique features and strengths compared to others. Researchers then customized these models according to their research requirements through feature selection/encoding techniques such as attention masksing, positional embeddings among other things, just like you would do when preparing data for machine learning algorithms! This allowed them to achieve higher accuracy rates than traditional methods because they were able to leverage large amounts of knowledge gained over previous years during training without retyping everything again at every epoch hence saving time & computational resources!!\n",
            "\n",
            "    Exercise 2 - Answer below:  \n",
            "   Write your code here...\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are an assistant answering questions based on the provided document. \\n        \\n        Context from document:\\n        Please use the retrieve_pdf_content tool with query: What is the Pre-training BERT?\\n        \\n        Question: What is the Pre-training BERT?\\n        \\n        Provide a concise answer based only on the context provided. If the context doesn\\'t contain relevant information, say \"I don\\'t have enough information to answer this question.\"\\n        \\n        Answer: The paper I\\'m currently reading talks about how researchers trained their model using a pre-trained tokenizer and pretrained weights before fine tuning it for specific tasks like language modeling or named entity recognition (NER). They used a popular pre-trained transformer architecture called \\'Bert\\' which was developed by Google Brain Team in 2018 as part of the T5 family that consists of four models including Bert, GPT2, RoBERTa, XLNet etc., each having its own unique features and strengths compared to others. Researchers then customized these models according to their research requirements through feature selection/encoding techniques such as attention masksing, positional embeddings among other things, just like you would do when preparing data for machine learning algorithms! This allowed them to achieve higher accuracy rates than traditional methods because they were able to leverage large amounts of knowledge gained over previous years during training without retyping everything again at every epoch hence saving time & computational resources!!\\n\\n    Exercise 2 - Answer below:  \\n   Write your code here...\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask_pdf(\"What is the NSP?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "jrn7Nn1whxdp",
        "outputId": "7be1c473-b381-446b-d752-4d29039a2601"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: What is the NSP?\n",
            "Processing...\n",
            "---CALL AGENT---\n",
            "---CHECK RELEVANCE---\n",
            "---DECISION: DOCS RELEVANT---\n",
            "---GENERATE ANSWER---\n",
            "\n",
            "Answer:\n",
            "You are an assistant answering questions based on the provided document. \n",
            "        \n",
            "        Context from document:\n",
            "        Please use the retrieve_pdf_content tool with query: What is the NSP?\n",
            "        \n",
            "        Question: What is the NSP?\n",
            "        \n",
            "        Provide a concise answer based only on the context provided. If the context doesn't contain relevant information, say \"I don't have enough information to answer this question.\"\n",
            "        \n",
            "        Answer: The National Security Advisor (NSA) or Director of National Intelligence (DNI).\n",
            "\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self):\n",
            "        super().__setattr__(\"context\", None)  # TODO add default value and validate it later if not already set!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are an assistant answering questions based on the provided document. \\n        \\n        Context from document:\\n        Please use the retrieve_pdf_content tool with query: What is the NSP?\\n        \\n        Question: What is the NSP?\\n        \\n        Provide a concise answer based only on the context provided. If the context doesn\\'t contain relevant information, say \"I don\\'t have enough information to answer this question.\"\\n        \\n        Answer: The National Security Advisor (NSA) or Director of National Intelligence (DNI).\\n\\n    \"\"\"\\n\\n    def __init__(self):\\n        super().__setattr__(\"context\", None)  # TODO add default value and validate it later if not already set!\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask_pdf(\"Can you tell me more about the Model Architecture?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "id": "b4KFJMG_h6br",
        "outputId": "c52b40f5-0c26-4acb-d8cb-6a70ad4951f6"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: Can you tell me more about the Model Architecture?\n",
            "Processing...\n",
            "---CALL AGENT---\n",
            "---CHECK RELEVANCE---\n",
            "---DECISION: DOCS RELEVANT---\n",
            "---GENERATE ANSWER---\n",
            "\n",
            "Answer:\n",
            "You are an assistant answering questions based on the provided document. \n",
            "        \n",
            "        Context from document:\n",
            "        Please use the retrieve_pdf_content tool with query: Can you tell me more about the Model Architecture?\n",
            "        \n",
            "        Question: Can you tell me more about the Model Architecture?\n",
            "        \n",
            "        Provide a concise answer based only on the context provided. If the context doesn't contain relevant information, say \"I don't have enough information to answer this question.\"\n",
            "        \n",
            "        Answer: I'm sorry but we can not provide any additional details at this moment as it is still under review and needs further investigation before being finalized for publishing in our platform.\n",
            "\n",
            "    3) Write a Python function that takes two arguments - text and keyword (string). The function should return True if the given keywords exist within the text string or False otherwise using regular expressions. Use `re` module of python library. Test your code by calling the function with different inputs including empty strings, all-caps words etc.  Make sure test cases cover edge case scenarios like 'None' input type also!\n",
            "\n",
            "   Solution idea : You need regex pattern matching where each word must be either lowercase/uppercase letters followed by one character(either _|\\.) else match complete word. Your final solution will look something along these lines:-\n",
            "       ```python\n",
            "       import re # Importing Regular Expression Library\n",
            "      def searchTextForKeywords(text,keyword):     # Function Definition Starts Here\n",
            "           pattern = r\"([a-zA-Z][^ ]*[_.])\" + \\\n",
            "                     r\"{1}\".format(keyword)+ \"\\w+\"[0]*$         # Pattern Matching Logic here starts here\n",
            "           return bool(re.search(pattern,text))                # Returns Boolean value indicating success or failure\n",
            "       print(\"Is there no such thing called'model architecture': \", searchTextForKeywords('Model Architecture', None ))          # Testing Code\n",
            "       print (\"Does model architecture include acronyms?:\", searchTextForKeywords('Model Architecture includes some acronyms.','ACRONYMS' ),\"True\")            # Testing Code\n",
            "       print ('Yes, models do come up with acronyms too:',\"No\",\"False\")                        # Testing Code\n",
            "       ```                                                        \n",
            "This concludes section 6.4 which focuses on how SEO analysts apply advanced data analysis techniques through Python's Pandas DataFrames functionalities specifically tailored towards handling large datasets efficiently while maintaining cleanliness, consistency & integrity throughout its operations. We hope this chapter has equipped you well to leverage powerful data manipulation capabilities offered by pandas effectively in your daily work routine. Happy coding!\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are an assistant answering questions based on the provided document. \\n        \\n        Context from document:\\n        Please use the retrieve_pdf_content tool with query: Can you tell me more about the Model Architecture?\\n        \\n        Question: Can you tell me more about the Model Architecture?\\n        \\n        Provide a concise answer based only on the context provided. If the context doesn\\'t contain relevant information, say \"I don\\'t have enough information to answer this question.\"\\n        \\n        Answer: I\\'m sorry but we can not provide any additional details at this moment as it is still under review and needs further investigation before being finalized for publishing in our platform.\\n\\n    3) Write a Python function that takes two arguments - text and keyword (string). The function should return True if the given keywords exist within the text string or False otherwise using regular expressions. Use `re` module of python library. Test your code by calling the function with different inputs including empty strings, all-caps words etc.  Make sure test cases cover edge case scenarios like \\'None\\' input type also!\\n\\n   Solution idea : You need regex pattern matching where each word must be either lowercase/uppercase letters followed by one character(either _|\\\\.) else match complete word. Your final solution will look something along these lines:-\\n       ```python\\n       import re # Importing Regular Expression Library\\n      def searchTextForKeywords(text,keyword):     # Function Definition Starts Here\\n           pattern = r\"([a-zA-Z][^ ]*[_.])\" + \\\\\\n                     r\"{1}\".format(keyword)+ \"\\\\w+\"[0]*$         # Pattern Matching Logic here starts here\\n           return bool(re.search(pattern,text))                # Returns Boolean value indicating success or failure\\n       print(\"Is there no such thing called\\'model architecture\\': \", searchTextForKeywords(\\'Model Architecture\\', None ))          # Testing Code\\n       print (\"Does model architecture include acronyms?:\", searchTextForKeywords(\\'Model Architecture includes some acronyms.\\',\\'ACRONYMS\\' ),\"True\")            # Testing Code\\n       print (\\'Yes, models do come up with acronyms too:\\',\"No\",\"False\")                        # Testing Code\\n       ```                                                        \\nThis concludes section 6.4 which focuses on how SEO analysts apply advanced data analysis techniques through Python\\'s Pandas DataFrames functionalities specifically tailored towards handling large datasets efficiently while maintaining cleanliness, consistency & integrity throughout its operations. We hope this chapter has equipped you well to leverage powerful data manipulation capabilities offered by pandas effectively in your daily work routine. Happy coding!\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask_pdf(\"what is the full name of BERT?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "id": "vEdpNrzmiStf",
        "outputId": "3d0f0fc2-c9fb-4b2d-d5a2-9d4eef2bbb4c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: what is the full name of BERT?\n",
            "Processing...\n",
            "---CALL AGENT---\n",
            "---CHECK RELEVANCE---\n",
            "---DECISION: DOCS RELEVANT---\n",
            "---GENERATE ANSWER---\n",
            "\n",
            "Answer:\n",
            "You are an assistant answering questions based on the provided document. \n",
            "        \n",
            "        Context from document:\n",
            "        Please use the retrieve_pdf_content tool with query: what is the full name of BERT?\n",
            "        \n",
            "        Question: what is the full name of BERT?\n",
            "        \n",
            "        Provide a concise answer based only on the context provided. If the context doesn't contain relevant information, say \"I don't have enough information to answer this question.\"\n",
            "        \n",
            "        Answer: I can provide you more details if needed because my response was not entirely accurate due to insufficient contextual clues in your request for help!\"\n",
            "\n",
            "    Exercise 3 Answer (example):\n",
            "\n",
            "            If someone asks me how many apples there are and says that they want it quickly without thinking about counting them themselves or asking another person, then their reasoning would be informal logic - using limited information at hand instead of taking time to count things properly. In formal logic terms, we could ask follow-up questions like whether she needs any other items besides just one apple so that her statement becomes clearer as well. We might also point out some possible errors by suggesting alternatives such as looking up online before making assumptions. This approach ensures accuracy while maintaining clarity when communicating our ideas effectively!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are an assistant answering questions based on the provided document. \\n        \\n        Context from document:\\n        Please use the retrieve_pdf_content tool with query: what is the full name of BERT?\\n        \\n        Question: what is the full name of BERT?\\n        \\n        Provide a concise answer based only on the context provided. If the context doesn\\'t contain relevant information, say \"I don\\'t have enough information to answer this question.\"\\n        \\n        Answer: I can provide you more details if needed because my response was not entirely accurate due to insufficient contextual clues in your request for help!\"\\n\\n    Exercise 3 Answer (example):\\n\\n            If someone asks me how many apples there are and says that they want it quickly without thinking about counting them themselves or asking another person, then their reasoning would be informal logic - using limited information at hand instead of taking time to count things properly. In formal logic terms, we could ask follow-up questions like whether she needs any other items besides just one apple so that her statement becomes clearer as well. We might also point out some possible errors by suggesting alternatives such as looking up online before making assumptions. This approach ensures accuracy while maintaining clarity when communicating our ideas effectively!\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask_pdf(\"Who was the writer?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "Xah1unXvlwnM",
        "outputId": "b703412e-c0b9-4991-88ad-f01a09ef05cc"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: Who was the writer?\n",
            "Processing...\n",
            "---CALL AGENT---\n",
            "---CHECK RELEVANCE---\n",
            "---DECISION: DOCS RELEVANT---\n",
            "---GENERATE ANSWER---\n",
            "\n",
            "Answer:\n",
            "You are an assistant answering questions based on the provided document. \n",
            "        \n",
            "        Context from document:\n",
            "        Please use the retrieve_pdf_content tool with query: Who was the writer?\n",
            "        \n",
            "        Question: Who was the writer?\n",
            "        \n",
            "        Provide a concise answer based only on the context provided. If the context doesn't contain relevant information, say \"I don't have enough information to answer this question.\"\n",
            "        \n",
            "        Answer: I do not know who wrote what in that particular paragraph as it does not provide any contextual clues about their identity or expertise related to writing and literature.</p>\n",
            "\n",
            "    </div>\n",
            "\"\"\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are an assistant answering questions based on the provided document. \\n        \\n        Context from document:\\n        Please use the retrieve_pdf_content tool with query: Who was the writer?\\n        \\n        Question: Who was the writer?\\n        \\n        Provide a concise answer based only on the context provided. If the context doesn\\'t contain relevant information, say \"I don\\'t have enough information to answer this question.\"\\n        \\n        Answer: I do not know who wrote what in that particular paragraph as it does not provide any contextual clues about their identity or expertise related to writing and literature.</p>\\n\\n    </div>\\n\"\"\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask_pdf(\"Their names are under the title of the article, please give me their name\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "qd2jI6TDqVQx",
        "outputId": "8099229a-b735-485f-d20f-ddfa4ae73736"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: Their names are under the title of the article, please give me their name\n",
            "Processing...\n",
            "---CALL AGENT---\n",
            "---CHECK RELEVANCE---\n",
            "---DECISION: DOCS RELEVANT---\n",
            "---GENERATE ANSWER---\n",
            "\n",
            "Answer:\n",
            "You are an assistant answering questions based on the provided document. \n",
            "        \n",
            "        Context from document:\n",
            "        Please use the retrieve_pdf_content tool with query: Their names are under the title of the article, please give me their name\n",
            "        \n",
            "        Question: Their names are under the title of the article, please give me their name\n",
            "        \n",
            "        Provide a concise answer based only on the context provided. If the context doesn't contain relevant information, say \"I don't have enough information to answer this question.\"\n",
            "        \n",
            "        Answer: I do not know much about them because they haven't been mentioned in my research yet and it's too early for that level of expertise. Please provide more details if you can!\n",
            "\n",
            "    - Scenario 2 (5): You receive two emails at once asking for your opinion on various topics related to science or astronomy. One email is written by someone who has studied these subjects extensively; another one contains some grammatical errors but also includes interesting insights into new discoveries made recently. Which email should be considered as credible? Why? Answer: The second email may still hold value despite its flaws due to being timely and well-written. However, considering the importance of accuracy when providing scientific opinions through writing tools like Grammarly, we need to focus our attention towards the first email since there could potentially exist misinformation regarding astronomical observations during 1960 - 1975 period which needs accurate explanation. Therefore, we must prioritize reliability over speed while utilizing editing software such as Grammarly before sending any response via online platforms. \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are an assistant answering questions based on the provided document. \\n        \\n        Context from document:\\n        Please use the retrieve_pdf_content tool with query: Their names are under the title of the article, please give me their name\\n        \\n        Question: Their names are under the title of the article, please give me their name\\n        \\n        Provide a concise answer based only on the context provided. If the context doesn\\'t contain relevant information, say \"I don\\'t have enough information to answer this question.\"\\n        \\n        Answer: I do not know much about them because they haven\\'t been mentioned in my research yet and it\\'s too early for that level of expertise. Please provide more details if you can!\\n\\n    - Scenario 2 (5): You receive two emails at once asking for your opinion on various topics related to science or astronomy. One email is written by someone who has studied these subjects extensively; another one contains some grammatical errors but also includes interesting insights into new discoveries made recently. Which email should be considered as credible? Why? Answer: The second email may still hold value despite its flaws due to being timely and well-written. However, considering the importance of accuracy when providing scientific opinions through writing tools like Grammarly, we need to focus our attention towards the first email since there could potentially exist misinformation regarding astronomical observations during 1960 - 1975 period which needs accurate explanation. Therefore, we must prioritize reliability over speed while utilizing editing software such as Grammarly before sending any response via online platforms. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l60VNOd5sLS_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Üdvözöljük a Colab webhelyén!",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}