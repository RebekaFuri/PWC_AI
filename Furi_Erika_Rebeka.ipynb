{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "714e46db-5ab9-4f31-8987-1eb0db45d0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.30.0 requires packaging<24,>=16.8, but you have packaging 24.2 which is incompatible.\n",
      "streamlit 1.30.0 requires protobuf<5,>=3.20, but you have protobuf 5.29.4 which is incompatible.\n",
      "streamlit 1.30.0 requires tenacity<9,>=8.1.0, but you have tenacity 9.1.2 which is incompatible.\n",
      "tensorflow-intel 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# 1. Környezeti beállítások\n",
    "!pip install langchain langgraph faiss-cpu sentence-transformers transformers pypdf tqdm -q langchain-community langchain-huggingface langchain_text_splitters chromadb pymupdf pdf2image Pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c898150c-173c-4871-b531-20f799f963b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import tempfile\n",
    "from typing import Annotated, Literal, Sequence, TypedDict\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34364dbd-2cb0-4d01-982f-8d928dc624e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document loading and processing\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.tools.retriever import create_retriever_tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f336b3d6-7a39-44ed-9c77-08b9579a8e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings and model\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73d183dd-6ab8-4c20-8d1b-f27c1c0d95b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up document processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\furir\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# LangGraph components\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "\n",
    "print(\"Setting up document processing...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0092fa8f-6cfb-4779-9da1-bca4cd28b6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 56 pages from PDF\n",
      "Split into 285 chunks\n"
     ]
    }
   ],
   "source": [
    "# 3. PDF feldolgozás és chunkolás\n",
    "pdf_path = \"Füri_Erika_Rebeka_PSZK.pdf\" \n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} pages from PDF\")\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(documents)\n",
    "print(f\"Split into {len(doc_splits)} chunks\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "faf6f93c-bf19-4b16-a9b1-537a951658dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Embedding és indexelés\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"pdf-rag-chroma\",\n",
    "    embedding=embeddings\n",
    ")\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_pdf_content\",\n",
    "    \"Search and return information from the PDF document based on the query.\"\n",
    ")\n",
    "tools = [retriever_tool]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56a239d7-d91e-4c9c-90db-386c384a70ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading microsoft/phi-1_5 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "#5. Set up the phi-1.5 model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "print(\"Loading microsoft/phi-1_5 model...\")\n",
    "model_id = \"microsoft/phi-1_5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9167ea0e-a932-484a-9a16-5d03cbfce9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text generation pipeline\n",
    "phi_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,  # Enable sampling\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.2,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36fb7e6f-186d-4252-9197-5c582831f1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HuggingFacePipeline LangChain wrapper\n",
    "phi_llm = HuggingFacePipeline(pipeline=phi_pipeline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e56b0989-9322-404a-9d44-de42eb529fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent state\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "686dfaf3-93fe-4185-9660-3b538dc87cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define edge functions\n",
    "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "    \"\"\"\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "    \n",
    "    # Data model for grading\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "    \n",
    "    # Prompt for relevance checking\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are assessing if a document is relevant to a question.\n",
    "        Document: {context}\n",
    "        Question: {question}\n",
    "        If the document contains information related to the question, say 'yes', otherwise say 'no'.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "    \n",
    "    # Simple relevance check with phi model\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "    docs = last_message.content\n",
    "    \n",
    "    # Use phi model to check relevance\n",
    "    inputs = prompt.format(question=question, context=docs)\n",
    "    result = phi_llm.invoke(inputs)\n",
    "    \n",
    "    # Extract yes/no from result\n",
    "    if \"yes\" in result.lower():\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        return \"rewrite\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b26b5a6f-9736-4f24-820d-972046c9045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define node functions\n",
    "def agent(state):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current state.\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Create a prompt to decide if retrieval is needed\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"Based on the question, do you need to search for information in the document?\n",
    "        Question: {question}\n",
    "        Answer 'Yes' if you need to search, or provide a direct answer if you don't need to search.\"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "    \n",
    "    question = messages[0].content\n",
    "    inputs = prompt.format(question=question)\n",
    "    result = phi_llm.invoke(inputs)\n",
    "    \n",
    "    # Check if retrieval is needed based on the response\n",
    "    if \"yes\" in result.lower():\n",
    "        # Return a tool call like message\n",
    "        return {\"messages\": [HumanMessage(content=f\"Please use the retrieve_pdf_content tool with query: {question}\")]}\n",
    "    else:\n",
    "        # Return a direct answer\n",
    "        return {\"messages\": [HumanMessage(content=result)]}\n",
    "\n",
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "    \"\"\"\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"Please rewrite this question to make it more specific and searchable: \n",
    "        Question: {question}\n",
    "        Improved question:\"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "    \n",
    "    inputs = prompt.format(question=question)\n",
    "    result = phi_llm.invoke(inputs)\n",
    "    \n",
    "    return {\"messages\": [HumanMessage(content=result)]}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer based on retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE ANSWER---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "    docs = last_message.content\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are an assistant answering questions based on the provided document. \n",
    "        \n",
    "        Context from document:\n",
    "        {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Provide a concise answer based only on the context provided. If the context doesn't contain relevant information, say \"I don't have enough information to answer this question.\"\n",
    "        \n",
    "        Answer:\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "    \n",
    "    # Chain\n",
    "    inputs = prompt.format(context=docs, question=question)\n",
    "    result = phi_llm.invoke(inputs)\n",
    "    \n",
    "    return {\"messages\": [HumanMessage(content=result)]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6c2f3bb-0f07-4f3a-aa34-71695ad91b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the retrieval agent graph...\n"
     ]
    }
   ],
   "source": [
    "# Build the graph\n",
    "print(\"Building the retrieval agent graph...\")\n",
    "workflow = StateGraph(AgentState)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "56ccadf8-e7b5-4df6-8faa-204729d61fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x18b4e920850>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the nodes\n",
    "workflow.add_node(\"agent\", agent)\n",
    "retrieve = ToolNode([retriever_tool])\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"rewrite\", rewrite)\n",
    "workflow.add_node(\"generate\", generate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5fdc272-e88c-4cba-b2db-ba3b15b532d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x18b4e920850>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the edges\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    # More simplified condition for phi model\n",
    "    lambda x: \"retrieve_pdf_content\" in x[\"messages\"][-1].content if x[\"messages\"] else False,\n",
    "    {\n",
    "        True: \"retrieve\",\n",
    "        False: END\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd43b7f8-5063-4d87-abf1-e88826f01002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x18b4e920850>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Edges after retrieval\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    grade_documents,\n",
    "    {\n",
    "        \"generate\": \"generate\",\n",
    "        \"rewrite\": \"rewrite\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "16c81872-63e2-4481-82f5-0e31d762cd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the graph\n",
    "graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "77931ff7-767a-46f8-a929-8e8f1e3fcc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to ask questions to the PDF\n",
    "def ask_pdf(question):\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(\"Processing...\")\n",
    "    \n",
    "    inputs = {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=question),\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    response = \"\"\n",
    "    # Process the graph and get results\n",
    "    for output in graph.stream(inputs):\n",
    "        for key, value in output.items():\n",
    "            if key == \"generate\":\n",
    "                response = value[\"messages\"][0].content\n",
    "    \n",
    "    print(\"\\nAnswer:\")\n",
    "    print(response)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "874fa13a-e627-45a0-a7c9-8095bd1efb92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Your PDF is ready for questions!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Your PDF is ready for questions!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b6a67b75-25d7-416d-b2a1-a44bd4546b82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Mi a szűk MI?\n",
      "Processing...\n",
      "---CALL AGENT---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Last message is not an AIMessage",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Ask a question to the PDF (replace with your question)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ask_pdf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMi a szűk MI?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[47], line 14\u001b[0m, in \u001b[0;36mask_pdf\u001b[1;34m(question)\u001b[0m\n\u001b[0;32m     12\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Process the graph and get results\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mstream(inputs):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerate\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2441\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[0m\n\u001b[0;32m   2435\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   2436\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[0;32m   2437\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   2438\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   2439\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[0;32m   2440\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[1;32m-> 2441\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[0;32m   2442\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m   2443\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m   2444\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[0;32m   2445\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[0;32m   2446\u001b[0m         ):\n\u001b[0;32m   2447\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[0;32m   2448\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[0;32m   2449\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\runner.py:153\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m    151\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m     run_with_retry(\n\u001b[0;32m    154\u001b[0m         t,\n\u001b[0;32m    155\u001b[0m         retry_policy,\n\u001b[0;32m    156\u001b[0m         configurable\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    157\u001b[0m             CONFIG_KEY_CALL: partial(\n\u001b[0;32m    158\u001b[0m                 _call,\n\u001b[0;32m    159\u001b[0m                 weakref\u001b[38;5;241m.\u001b[39mref(t),\n\u001b[0;32m    160\u001b[0m                 retry\u001b[38;5;241m=\u001b[39mretry_policy,\n\u001b[0;32m    161\u001b[0m                 futures\u001b[38;5;241m=\u001b[39mweakref\u001b[38;5;241m.\u001b[39mref(futures),\n\u001b[0;32m    162\u001b[0m                 schedule_task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschedule_task,\n\u001b[0;32m    163\u001b[0m                 submit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit,\n\u001b[0;32m    164\u001b[0m                 reraise\u001b[38;5;241m=\u001b[39mreraise,\n\u001b[0;32m    165\u001b[0m             ),\n\u001b[0;32m    166\u001b[0m         },\n\u001b[0;32m    167\u001b[0m     )\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39minvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langgraph\\utils\\runnable.py:623\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m--> 623\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langgraph\\utils\\runnable.py:377\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 377\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langgraph\\prebuilt\\tool_node.py:235\u001b[0m, in \u001b[0;36mToolNode._func\u001b[1;34m(self, input, config, store)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_func\u001b[39m(\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28minput\u001b[39m: Union[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    233\u001b[0m     store: Optional[BaseStore],\n\u001b[0;32m    234\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 235\u001b[0m     tool_calls, input_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_input(\u001b[38;5;28minput\u001b[39m, store)\n\u001b[0;32m    236\u001b[0m     config_list \u001b[38;5;241m=\u001b[39m get_config_list(config, \u001b[38;5;28mlen\u001b[39m(tool_calls))\n\u001b[0;32m    237\u001b[0m     input_types \u001b[38;5;241m=\u001b[39m [input_type] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(tool_calls)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langgraph\\prebuilt\\tool_node.py:446\u001b[0m, in \u001b[0;36mToolNode._parse_input\u001b[1;34m(self, input, store)\u001b[0m\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo message found in input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, AIMessage):\n\u001b[1;32m--> 446\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLast message is not an AIMessage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    448\u001b[0m tool_calls \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minject_tool_args(call, \u001b[38;5;28minput\u001b[39m, store) \u001b[38;5;28;01mfor\u001b[39;00m call \u001b[38;5;129;01min\u001b[39;00m message\u001b[38;5;241m.\u001b[39mtool_calls\n\u001b[0;32m    450\u001b[0m ]\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tool_calls, input_type\n",
      "\u001b[1;31mValueError\u001b[0m: Last message is not an AIMessage",
      "\u001b[0mDuring task with name 'retrieve' and id '4a5ef501-a3f8-9157-4b2c-fe5179749b54'"
     ]
    }
   ],
   "source": [
    "# Ask a question to the PDF (replace with your question)\n",
    "ask_pdf(\"Mi a szűk MI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d8a7d4-c205-4051-a272-49fcdff8bc4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
